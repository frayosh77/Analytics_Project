---
title: "Project_RMark_Ver2"
author: "Frayosh Khursetjee"
date: "5 March 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Understanding the Data

The Dependent Variable is a Categorical Variable and the remainder variables are a combination of continuous and Categorical variables.  40% of the Variables are populated with Blank values hence they were all removed.



```{r basic_1, echo=TRUE, warning=FALSE, message=FALSE}
library(ggplot2)
library(rpart)
library(rpart.plot)
library(caret)
library(randomForest)
library(dplyr)

```


```{r basic_2, echo=TRUE}

pml.training <- read.csv("//inmbfp1/redirect1/g551739/My Documents/R_Code/Machine_Learning/pml-training.csv", stringsAsFactors=FALSE)
dim(pml.training)
pml.testing <- read.csv("//inmbfp1/redirect1/g551739/My Documents/R_Code/Machine_Learning/pml-testing.csv", stringsAsFactors=FALSE)
```

### Pre-Processing and drawing nuances

The training Data had only two sets of Categorical Variables: The Dependent Variable (Classe) and (User_name). In Order to Understand the Relationship between two categorical variables I started off with a simple Pearsonâ€™s Chi Squared Test. Below given is the result.

Ho (Null Hypothesis): Considered Categorical Variables are independent.

Ha (Alternate Hypothesis): Considered Categorical Variables are in some way dependent

The given P-Value < 0.05 (assumed Alfa Value with a 95% confident Band). Thereby implying a strong correlation between Classe and User_Name.


```{r basic_3}
klm <- table(pml.training$user_name, pml.training$classe)
chisq.test(klm)
```

 Checking for One Sided Data & Removing unnecesary Variables New Window & Time Stamp Variables
 
```{r basic_4}

train <- pml.training
train <- select(pml.training, -c(2,3,4,5))
ggplot(pml.training,aes(x=new_window)) + geom_bar(fill = "blue")

```

Checking for Multicollinearity and removing correlated variables

```{r basic_5}

M <- abs(cor(train[,-c(1,55)]))
diag(M) <- 0
ll <- as.data.frame(which(M > 0.9,arr.ind=T))
rownames(ll)
train <- select(train, -c(roll_belt,pitch_belt,accel_belt_y,gyros_arm_x,accel_belt_z,gyros_dumbbell_x, gyros_dumbbell_z, magnet_belt_x, magnet_arm_x, magnet_arm_z, gyros_forearm_z,accel_dumbbell_z,accel_dumbbell_x))

```

### Selecting the Model

Since we are predicting Categorical values and not continuous variables at a point in time, time-series modelling is out of the question. For Categorical Variable Predicting the best bet would be a CART model using the rpart Package and Random Forest. 

We first Generate a CART Model using K-fold Validation with 5 Buckets. For a better analysis we have pre-processed the Data. i.e., subtract by the mean and dividing by the Standard Deviation

```{r basic_6}
control <- trainControl(method = "cv", number = 5) 
rpart_model <- train(classe ~ ., data = train, method = "rpart", trControl = control, preProcess = c("center","scale"))
rpart_model
```

Due to limited accuracy on the CART Model we shift to the Random Forest. Basically making mutiple Regression trees till we get a reduced error on one.

```{r basic_7}
train$classe = as.factor(train$classe)
train$user_name = as.factor(train$user_name)
rf_model <- randomForest(classe ~ ., data = train)
plot(rf_model)
```

Predicting on the given test set

```{r basic_9}
test <- pml.testing
test <- select(pml.testing, -c(2,3,4,5))
test <- select(test, -c(roll_belt,pitch_belt,accel_belt_y,gyros_arm_x,accel_belt_z,gyros_dumbbell_x, gyros_dumbbell_z, magnet_belt_x, magnet_arm_x, magnet_arm_z, gyros_forearm_z,accel_dumbbell_z,accel_dumbbell_x))
test$user_name = as.factor(test$user_name)
predict(rf_model,test)
```





